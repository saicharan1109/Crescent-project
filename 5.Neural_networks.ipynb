{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bda2af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder,Normalizer,MinMaxScaler,LabelEncoder,StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import classification_report,roc_auc_score,roc_curve\n",
    "from sklearn.impute import KNNImputer,SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30552742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROMISE_STATUS_O</th>\n",
       "      <th>EXTENSION_ELIGIBLE_Y</th>\n",
       "      <th>F0223_NUMBER_OF_EXTENSIONS</th>\n",
       "      <th>lastchannel_Z</th>\n",
       "      <th>REM_TM_RAT_OBS</th>\n",
       "      <th>ApplicantFICOScore</th>\n",
       "      <th>Appl_Debt</th>\n",
       "      <th>F0066_CONTACT_NO_PROMISE_COUNT</th>\n",
       "      <th>F0315_DAYS_SINCE_LAST_PROMISE_1</th>\n",
       "      <th>F0065_PROMISES_TAKEN_COUNT</th>\n",
       "      <th>...</th>\n",
       "      <th>DepreciationRate</th>\n",
       "      <th>Appl_Income</th>\n",
       "      <th>PTI_BOOKED</th>\n",
       "      <th>SalesTax</th>\n",
       "      <th>DOWN_CASH</th>\n",
       "      <th>FINANCED_AMOUNT</th>\n",
       "      <th>F0089_PROMISE_AMT_1</th>\n",
       "      <th>F0180_CURRENT_INTEREST_RATE</th>\n",
       "      <th>BAL_RAT_TRN_6.1</th>\n",
       "      <th>DPD_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1075.05</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694261</td>\n",
       "      <td>4138.00</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>708.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15057.22</td>\n",
       "      <td>382.68</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1138.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776757</td>\n",
       "      <td>4166.67</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>20095.17</td>\n",
       "      <td>419.21</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>767.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>2833.33</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>1916.82</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>18764.82</td>\n",
       "      <td>432.01</td>\n",
       "      <td>0.1808</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>1071.20</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>4000.00</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>15077.00</td>\n",
       "      <td>361.22</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>726.64</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>2069.02</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>880.60</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>10200.54</td>\n",
       "      <td>280.21</td>\n",
       "      <td>0.2134</td>\n",
       "      <td>0.05</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1808.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776757</td>\n",
       "      <td>4991.09</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>1040.82</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>17252.97</td>\n",
       "      <td>437.95</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62934</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>720.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856943</td>\n",
       "      <td>2218.75</td>\n",
       "      <td>0.1728</td>\n",
       "      <td>841.68</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>15382.68</td>\n",
       "      <td>402.48</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>852.94</td>\n",
       "      <td>3.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629297</td>\n",
       "      <td>2810.34</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>872.19</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>15924.07</td>\n",
       "      <td>371.17</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.04</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1026.95</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629297</td>\n",
       "      <td>3704.74</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>1152.30</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>18715.30</td>\n",
       "      <td>500.00</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>860.65</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734352</td>\n",
       "      <td>3500.00</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.00</td>\n",
       "      <td>800.0</td>\n",
       "      <td>12305.00</td>\n",
       "      <td>136.03</td>\n",
       "      <td>0.2399</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62938 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROMISE_STATUS_O  EXTENSION_ELIGIBLE_Y  F0223_NUMBER_OF_EXTENSIONS  \\\n",
       "0                   0.0                   0.0                         3.0   \n",
       "1                   0.0                   1.0                         0.0   \n",
       "2                   0.0                   1.0                         0.0   \n",
       "3                   1.0                   0.0                         2.0   \n",
       "4                   0.0                   1.0                         0.0   \n",
       "...                 ...                   ...                         ...   \n",
       "62933               0.0                   0.0                         2.0   \n",
       "62934               1.0                   1.0                         0.0   \n",
       "62935               0.0                   1.0                         1.0   \n",
       "62936               0.0                   0.0                         4.0   \n",
       "62937               0.0                   0.0                         2.0   \n",
       "\n",
       "       lastchannel_Z  REM_TM_RAT_OBS  ApplicantFICOScore  Appl_Debt  \\\n",
       "0                1.0             0.0                 0.0    1075.05   \n",
       "1                1.0             0.0                 0.0    1138.75   \n",
       "2                1.0             0.0               551.0     767.83   \n",
       "3                1.0             0.0               468.0    1071.20   \n",
       "4                1.0             0.0               523.0     726.64   \n",
       "...              ...             ...                 ...        ...   \n",
       "62933            1.0             0.0               496.0    1808.27   \n",
       "62934            1.0             0.0               458.0     720.43   \n",
       "62935            1.0             0.0               567.0     852.94   \n",
       "62936            1.0             0.0               447.0    1026.95   \n",
       "62937            1.0             0.0                 0.0     860.65   \n",
       "\n",
       "       F0066_CONTACT_NO_PROMISE_COUNT  F0315_DAYS_SINCE_LAST_PROMISE_1  \\\n",
       "0                                12.0                              5.0   \n",
       "1                                 3.0                             45.0   \n",
       "2                                 0.0                              3.0   \n",
       "3                                14.0                            -11.0   \n",
       "4                                 2.0                             80.0   \n",
       "...                               ...                              ...   \n",
       "62933                             4.0                             17.0   \n",
       "62934                             1.0                            -16.0   \n",
       "62935                             3.0                            320.0   \n",
       "62936                            11.0                             31.0   \n",
       "62937                             2.0                             17.0   \n",
       "\n",
       "       F0065_PROMISES_TAKEN_COUNT  ...  DepreciationRate  Appl_Income  \\\n",
       "0                            25.0  ...          0.694261      4138.00   \n",
       "1                             3.0  ...          0.776757      4166.67   \n",
       "2                             3.0  ...          0.834200      2833.33   \n",
       "3                            10.0  ...          0.810160      4000.00   \n",
       "4                             7.0  ...          0.810160      2069.02   \n",
       "...                           ...  ...               ...          ...   \n",
       "62933                         7.0  ...          0.776757      4991.09   \n",
       "62934                         9.0  ...          0.856943      2218.75   \n",
       "62935                        13.0  ...          0.629297      2810.34   \n",
       "62936                        16.0  ...          0.629297      3704.74   \n",
       "62937                         1.0  ...          0.734352      3500.00   \n",
       "\n",
       "       PTI_BOOKED  SalesTax  DOWN_CASH  FINANCED_AMOUNT  F0089_PROMISE_AMT_1  \\\n",
       "0          0.0964    708.22        0.0         15057.22               382.68   \n",
       "1          0.1006      0.00     3500.0         20095.17               419.21   \n",
       "2          0.1525   1916.82     5000.0         18764.82               432.01   \n",
       "3          0.0903      0.00     3000.0         15077.00               361.22   \n",
       "4          0.1354    880.60     3000.0         10200.54               280.21   \n",
       "...           ...       ...        ...              ...                  ...   \n",
       "62933      0.0877   1040.82     2000.0         17252.97               437.95   \n",
       "62934      0.1728    841.68     2100.0         15382.68               402.48   \n",
       "62935      0.1321    872.19     1500.0         15924.07               371.17   \n",
       "62936      0.1338   1152.30     4000.0         18715.30               500.00   \n",
       "62937      0.0931      0.00      800.0         12305.00               136.03   \n",
       "\n",
       "       F0180_CURRENT_INTEREST_RATE  BAL_RAT_TRN_6.1  DPD_bool  \n",
       "0                           0.1999             0.01     False  \n",
       "1                           0.1425             0.03     False  \n",
       "2                           0.1808             0.03     False  \n",
       "3                           0.1970             0.02     False  \n",
       "4                           0.2134             0.05     False  \n",
       "...                            ...              ...       ...  \n",
       "62933                       0.1800             0.01     False  \n",
       "62934                       0.2140             0.03     False  \n",
       "62935                       0.1650             0.04     False  \n",
       "62936                       0.2425             0.00      True  \n",
       "62937                       0.2399             0.03     False  \n",
       "\n",
       "[62938 rows x 41 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../ready_to_model_data.csv\")\n",
    "df=df.drop(columns=\"Unnamed: 0\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5208304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROMISE_STATUS_O</th>\n",
       "      <th>EXTENSION_ELIGIBLE_Y</th>\n",
       "      <th>F0223_NUMBER_OF_EXTENSIONS</th>\n",
       "      <th>lastchannel_Z</th>\n",
       "      <th>REM_TM_RAT_OBS</th>\n",
       "      <th>ApplicantFICOScore</th>\n",
       "      <th>Appl_Debt</th>\n",
       "      <th>F0066_CONTACT_NO_PROMISE_COUNT</th>\n",
       "      <th>F0315_DAYS_SINCE_LAST_PROMISE_1</th>\n",
       "      <th>F0065_PROMISES_TAKEN_COUNT</th>\n",
       "      <th>...</th>\n",
       "      <th>DepreciationRate</th>\n",
       "      <th>Appl_Income</th>\n",
       "      <th>PTI_BOOKED</th>\n",
       "      <th>SalesTax</th>\n",
       "      <th>DOWN_CASH</th>\n",
       "      <th>FINANCED_AMOUNT</th>\n",
       "      <th>F0089_PROMISE_AMT_1</th>\n",
       "      <th>F0180_CURRENT_INTEREST_RATE</th>\n",
       "      <th>BAL_RAT_TRN_6.1</th>\n",
       "      <th>DPD_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1075.05</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694261</td>\n",
       "      <td>4138.00</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>708.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15057.22</td>\n",
       "      <td>382.68</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1138.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776757</td>\n",
       "      <td>4166.67</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>20095.17</td>\n",
       "      <td>419.21</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>767.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>2833.33</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>1916.82</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>18764.82</td>\n",
       "      <td>432.01</td>\n",
       "      <td>0.1808</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>1071.20</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>4000.00</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>15077.00</td>\n",
       "      <td>361.22</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>726.64</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>2069.02</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>880.60</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>10200.54</td>\n",
       "      <td>280.21</td>\n",
       "      <td>0.2134</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1808.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776757</td>\n",
       "      <td>4991.09</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>1040.82</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>17252.97</td>\n",
       "      <td>437.95</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62934</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>720.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856943</td>\n",
       "      <td>2218.75</td>\n",
       "      <td>0.1728</td>\n",
       "      <td>841.68</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>15382.68</td>\n",
       "      <td>402.48</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>852.94</td>\n",
       "      <td>3.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629297</td>\n",
       "      <td>2810.34</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>872.19</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>15924.07</td>\n",
       "      <td>371.17</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1026.95</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629297</td>\n",
       "      <td>3704.74</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>1152.30</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>18715.30</td>\n",
       "      <td>500.00</td>\n",
       "      <td>0.2425</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>860.65</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734352</td>\n",
       "      <td>3500.00</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.00</td>\n",
       "      <td>800.0</td>\n",
       "      <td>12305.00</td>\n",
       "      <td>136.03</td>\n",
       "      <td>0.2399</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62938 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROMISE_STATUS_O  EXTENSION_ELIGIBLE_Y  F0223_NUMBER_OF_EXTENSIONS  \\\n",
       "0                   0.0                   0.0                         3.0   \n",
       "1                   0.0                   1.0                         0.0   \n",
       "2                   0.0                   1.0                         0.0   \n",
       "3                   1.0                   0.0                         2.0   \n",
       "4                   0.0                   1.0                         0.0   \n",
       "...                 ...                   ...                         ...   \n",
       "62933               0.0                   0.0                         2.0   \n",
       "62934               1.0                   1.0                         0.0   \n",
       "62935               0.0                   1.0                         1.0   \n",
       "62936               0.0                   0.0                         4.0   \n",
       "62937               0.0                   0.0                         2.0   \n",
       "\n",
       "       lastchannel_Z  REM_TM_RAT_OBS  ApplicantFICOScore  Appl_Debt  \\\n",
       "0                1.0             0.0                 0.0    1075.05   \n",
       "1                1.0             0.0                 0.0    1138.75   \n",
       "2                1.0             0.0               551.0     767.83   \n",
       "3                1.0             0.0               468.0    1071.20   \n",
       "4                1.0             0.0               523.0     726.64   \n",
       "...              ...             ...                 ...        ...   \n",
       "62933            1.0             0.0               496.0    1808.27   \n",
       "62934            1.0             0.0               458.0     720.43   \n",
       "62935            1.0             0.0               567.0     852.94   \n",
       "62936            1.0             0.0               447.0    1026.95   \n",
       "62937            1.0             0.0                 0.0     860.65   \n",
       "\n",
       "       F0066_CONTACT_NO_PROMISE_COUNT  F0315_DAYS_SINCE_LAST_PROMISE_1  \\\n",
       "0                                12.0                              5.0   \n",
       "1                                 3.0                             45.0   \n",
       "2                                 0.0                              3.0   \n",
       "3                                14.0                            -11.0   \n",
       "4                                 2.0                             80.0   \n",
       "...                               ...                              ...   \n",
       "62933                             4.0                             17.0   \n",
       "62934                             1.0                            -16.0   \n",
       "62935                             3.0                            320.0   \n",
       "62936                            11.0                             31.0   \n",
       "62937                             2.0                             17.0   \n",
       "\n",
       "       F0065_PROMISES_TAKEN_COUNT  ...  DepreciationRate  Appl_Income  \\\n",
       "0                            25.0  ...          0.694261      4138.00   \n",
       "1                             3.0  ...          0.776757      4166.67   \n",
       "2                             3.0  ...          0.834200      2833.33   \n",
       "3                            10.0  ...          0.810160      4000.00   \n",
       "4                             7.0  ...          0.810160      2069.02   \n",
       "...                           ...  ...               ...          ...   \n",
       "62933                         7.0  ...          0.776757      4991.09   \n",
       "62934                         9.0  ...          0.856943      2218.75   \n",
       "62935                        13.0  ...          0.629297      2810.34   \n",
       "62936                        16.0  ...          0.629297      3704.74   \n",
       "62937                         1.0  ...          0.734352      3500.00   \n",
       "\n",
       "       PTI_BOOKED  SalesTax  DOWN_CASH  FINANCED_AMOUNT  F0089_PROMISE_AMT_1  \\\n",
       "0          0.0964    708.22        0.0         15057.22               382.68   \n",
       "1          0.1006      0.00     3500.0         20095.17               419.21   \n",
       "2          0.1525   1916.82     5000.0         18764.82               432.01   \n",
       "3          0.0903      0.00     3000.0         15077.00               361.22   \n",
       "4          0.1354    880.60     3000.0         10200.54               280.21   \n",
       "...           ...       ...        ...              ...                  ...   \n",
       "62933      0.0877   1040.82     2000.0         17252.97               437.95   \n",
       "62934      0.1728    841.68     2100.0         15382.68               402.48   \n",
       "62935      0.1321    872.19     1500.0         15924.07               371.17   \n",
       "62936      0.1338   1152.30     4000.0         18715.30               500.00   \n",
       "62937      0.0931      0.00      800.0         12305.00               136.03   \n",
       "\n",
       "       F0180_CURRENT_INTEREST_RATE  BAL_RAT_TRN_6.1  DPD_bool  \n",
       "0                           0.1999             0.01         0  \n",
       "1                           0.1425             0.03         0  \n",
       "2                           0.1808             0.03         0  \n",
       "3                           0.1970             0.02         0  \n",
       "4                           0.2134             0.05         0  \n",
       "...                            ...              ...       ...  \n",
       "62933                       0.1800             0.01         0  \n",
       "62934                       0.2140             0.03         0  \n",
       "62935                       0.1650             0.04         0  \n",
       "62936                       0.2425             0.00         1  \n",
       "62937                       0.2399             0.03         0  \n",
       "\n",
       "[62938 rows x 41 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DPD_bool\"]=df[\"DPD_bool\"].map({True:1,False:0})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c949e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df.drop(columns=[\"DPD_bool\",\"F0381_DAYS_DELINQUENT_360\",\"D5P_TOT_1\"]),df[\"DPD_bool\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1766e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_norm=scaler.fit_transform(X_train)\n",
    "X_test_norm =scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d38de321",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm=pd.DataFrame(X_train_norm,columns=X_train.columns)\n",
    "X_test_norm=pd.DataFrame(X_test_norm,columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a289b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78a9fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 0s 856us/step - loss: 0.6166 - accuracy: 0.6761 - false_negatives: 5354.0000\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 0s 818us/step - loss: 0.4465 - accuracy: 0.8586 - false_negatives: 4338.0000\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 0s 898us/step - loss: 0.3264 - accuracy: 0.8899 - false_negatives: 3230.0000\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 0s 836us/step - loss: 0.2521 - accuracy: 0.9090 - false_negatives: 2601.0000\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 0s 883us/step - loss: 0.2027 - accuracy: 0.9273 - false_negatives: 2148.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f983a2e79d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We build a NN with two hidden layers, and 6 nodes in each hidden layer.\n",
    "\n",
    "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "classifier = Sequential()\n",
    "\n",
    "# add the first hidden layer\n",
    "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "\n",
    "# add the second hidden layer\n",
    "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "\n",
    "# add the output layer\n",
    "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "\n",
    "# add additional parameters\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'FalseNegatives'])\n",
    "\n",
    "# train the model\n",
    "classifier.fit(X_train_norm,y_train,batch_size=1000,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "858ab39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-88f405be262e>:20: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn=build_classifier)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 975us/step - loss: 0.7098 - accuracy: 0.5103\n",
      "46/46 [==============================] - 0s 838us/step - loss: 0.7484 - accuracy: 0.4985\n",
      "46/46 [==============================] - 0s 827us/step - loss: 0.6269 - accuracy: 0.6597\n",
      "46/46 [==============================] - 0s 911us/step - loss: 0.7924 - accuracy: 0.4904\n",
      "46/46 [==============================] - 0s 881us/step - loss: 0.5864 - accuracy: 0.7190\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.7485 - accuracy: 0.5131\n",
      "46/46 [==============================] - 0s 854us/step - loss: 0.7156 - accuracy: 0.5767\n",
      "46/46 [==============================] - 0s 862us/step - loss: 0.4678 - accuracy: 0.8203\n",
      "46/46 [==============================] - 0s 836us/step - loss: 1.2879 - accuracy: 0.2086\n",
      "46/46 [==============================] - 0s 835us/step - loss: 0.5758 - accuracy: 0.7641\n",
      "46/46 [==============================] - 0s 860us/step - loss: 0.5190 - accuracy: 0.8084\n",
      "46/46 [==============================] - 0s 827us/step - loss: 0.5194 - accuracy: 0.8147\n",
      "46/46 [==============================] - 0s 926us/step - loss: 0.6068 - accuracy: 0.6907\n",
      "46/46 [==============================] - 1s 957us/step - loss: 0.8169 - accuracy: 0.4911\n",
      "46/46 [==============================] - 0s 849us/step - loss: 0.6635 - accuracy: 0.6625\n",
      "46/46 [==============================] - 0s 913us/step - loss: 0.7148 - accuracy: 0.4728\n",
      "46/46 [==============================] - 0s 843us/step - loss: 0.4093 - accuracy: 0.8462\n",
      "46/46 [==============================] - 0s 864us/step - loss: 0.7868 - accuracy: 0.5088\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.6043 - accuracy: 0.7974\n",
      "46/46 [==============================] - 0s 850us/step - loss: 0.4936 - accuracy: 0.8211\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.8466\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.8464\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8383 - accuracy: 0.3087\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.6661\n",
      "23/23 [==============================] - 1s 1ms/step - loss: 0.9948 - accuracy: 0.2203\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7833 - accuracy: 0.4590\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7558 - accuracy: 0.4460\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.2150 - accuracy: 0.1785\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.8477\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5880 - accuracy: 0.7723\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6409 - accuracy: 0.7691\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6414 - accuracy: 0.7799\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8537 - accuracy: 0.4110\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6477 - accuracy: 0.8190\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.0775 - accuracy: 0.1956\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6689 - accuracy: 0.6642\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8275 - accuracy: 0.2040\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8289 - accuracy: 0.4344\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7562 - accuracy: 0.4748\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.0074 - accuracy: 0.1894\n",
      "51/51 [==============================] - 0s 884us/step - loss: 0.7843 - accuracy: 0.3627\n",
      "0.715770 (0.157471) with: {'batch_size': 1000, 'nb_epoch': 20, 'optimizer': 'adam'}\n",
      "0.789255 (0.075476) with: {'batch_size': 1000, 'nb_epoch': 10, 'optimizer': 'adam'}\n",
      "0.609752 (0.239107) with: {'batch_size': 2000, 'nb_epoch': 20, 'optimizer': 'adam'}\n",
      "0.560715 (0.242866) with: {'batch_size': 2000, 'nb_epoch': 10, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    # first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "    classifier = Sequential()\n",
    "    # add the first hidden layer\n",
    "    classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the second hidden layer\n",
    "    classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the output layer\n",
    "    classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "    # compiling the NN\n",
    "    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "# create a dictionary of hyper-parameters to optimize\n",
    "parameters = {'batch_size':[1000,2000], 'nb_epoch':[20,10],'optimizer':['adam']}\n",
    "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv=10)\n",
    "grid_search = grid_search.fit(X_train_norm,y_train)\n",
    "\n",
    "best_parameters = grid_search.best_params_ \n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09897fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000, 'nb_epoch': 10, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "385e7471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7892552135054618"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5603da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.wrappers.scikit_learn.KerasClassifier at 0x7f98430be670>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afd2abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaaeda1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.4180 - accuracy: 0.8558 - val_loss: 0.3828 - val_accuracy: 0.8737\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8713 - val_loss: 0.3363 - val_accuracy: 0.8856\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8821 - val_loss: 0.3033 - val_accuracy: 0.8936\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2975 - accuracy: 0.8896 - val_loss: 0.2781 - val_accuracy: 0.8991\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8959 - val_loss: 0.2583 - val_accuracy: 0.9055\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.9026 - val_loss: 0.2417 - val_accuracy: 0.9102\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9072 - val_loss: 0.2273 - val_accuracy: 0.9150\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9121 - val_loss: 0.2145 - val_accuracy: 0.9214\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9167 - val_loss: 0.2032 - val_accuracy: 0.9253\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2023 - accuracy: 0.9200 - val_loss: 0.1933 - val_accuracy: 0.9285\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6375 - accuracy: 0.7088 - val_loss: 0.5967 - val_accuracy: 0.7458\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7589 - val_loss: 0.5448 - val_accuracy: 0.7808\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5374 - accuracy: 0.7889 - val_loss: 0.5026 - val_accuracy: 0.8102\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.8114 - val_loss: 0.4679 - val_accuracy: 0.8245\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4688 - accuracy: 0.8254 - val_loss: 0.4387 - val_accuracy: 0.8348\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.8328 - val_loss: 0.4134 - val_accuracy: 0.8427\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8378 - val_loss: 0.3902 - val_accuracy: 0.8491\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.8433 - val_loss: 0.3685 - val_accuracy: 0.8507\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8469 - val_loss: 0.3483 - val_accuracy: 0.8539\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8504 - val_loss: 0.3299 - val_accuracy: 0.8594\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.6270 - accuracy: 0.6144 - val_loss: 0.5854 - val_accuracy: 0.6680\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7059 - val_loss: 0.5237 - val_accuracy: 0.7339\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4980 - accuracy: 0.7704 - val_loss: 0.4729 - val_accuracy: 0.7736\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4481 - accuracy: 0.8084 - val_loss: 0.4333 - val_accuracy: 0.8022\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8356 - val_loss: 0.4003 - val_accuracy: 0.8411\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8571 - val_loss: 0.3735 - val_accuracy: 0.8650\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8725 - val_loss: 0.3506 - val_accuracy: 0.8697\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8822 - val_loss: 0.3304 - val_accuracy: 0.8769\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8900 - val_loss: 0.3130 - val_accuracy: 0.8856\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8959 - val_loss: 0.2970 - val_accuracy: 0.8920\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.0458 - accuracy: 0.2261 - val_loss: 0.9581 - val_accuracy: 0.2740\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9288 - accuracy: 0.3142 - val_loss: 0.8603 - val_accuracy: 0.3685\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8409 - accuracy: 0.4005 - val_loss: 0.7894 - val_accuracy: 0.4694\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7777 - accuracy: 0.4925 - val_loss: 0.7383 - val_accuracy: 0.5663\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7308 - accuracy: 0.5791 - val_loss: 0.7005 - val_accuracy: 0.6458\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6953 - accuracy: 0.6511 - val_loss: 0.6699 - val_accuracy: 0.7156\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.7087 - val_loss: 0.6442 - val_accuracy: 0.7665\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6412 - accuracy: 0.7610 - val_loss: 0.6211 - val_accuracy: 0.8014\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.8062 - val_loss: 0.5998 - val_accuracy: 0.8499\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5970 - accuracy: 0.8433 - val_loss: 0.5787 - val_accuracy: 0.8761\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 31ms/step - loss: 0.7796 - accuracy: 0.4294 - val_loss: 0.7323 - val_accuracy: 0.4766\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7103 - accuracy: 0.5441 - val_loss: 0.6738 - val_accuracy: 0.6005\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6564 - accuracy: 0.6627 - val_loss: 0.6257 - val_accuracy: 0.7315\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6114 - accuracy: 0.7562 - val_loss: 0.5828 - val_accuracy: 0.8070\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.8133 - val_loss: 0.5425 - val_accuracy: 0.8499\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5316 - accuracy: 0.8491 - val_loss: 0.5032 - val_accuracy: 0.8761\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8751 - val_loss: 0.4638 - val_accuracy: 0.8967\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8898 - val_loss: 0.4254 - val_accuracy: 0.9055\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.9032 - val_loss: 0.3891 - val_accuracy: 0.9182\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.9116 - val_loss: 0.3553 - val_accuracy: 0.9269\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.1172 - accuracy: 0.1788 - val_loss: 1.0666 - val_accuracy: 0.1914\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9923 - accuracy: 0.2149 - val_loss: 0.9553 - val_accuracy: 0.2478\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9001 - accuracy: 0.2698 - val_loss: 0.8755 - val_accuracy: 0.3058\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8323 - accuracy: 0.3347 - val_loss: 0.8176 - val_accuracy: 0.3828\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7819 - accuracy: 0.4073 - val_loss: 0.7726 - val_accuracy: 0.4647\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7418 - accuracy: 0.4903 - val_loss: 0.7359 - val_accuracy: 0.5528\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.5774 - val_loss: 0.7048 - val_accuracy: 0.6418\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.6580 - val_loss: 0.6764 - val_accuracy: 0.7077\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6512 - accuracy: 0.7286 - val_loss: 0.6497 - val_accuracy: 0.7538\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6252 - accuracy: 0.7840 - val_loss: 0.6240 - val_accuracy: 0.7959\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.8722 - accuracy: 0.3867 - val_loss: 0.8291 - val_accuracy: 0.4345\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7998 - accuracy: 0.4721 - val_loss: 0.7668 - val_accuracy: 0.5028\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7421 - accuracy: 0.5554 - val_loss: 0.7173 - val_accuracy: 0.5830\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6963 - accuracy: 0.6347 - val_loss: 0.6779 - val_accuracy: 0.6664\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6594 - accuracy: 0.7075 - val_loss: 0.6454 - val_accuracy: 0.7276\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6284 - accuracy: 0.7686 - val_loss: 0.6171 - val_accuracy: 0.7776\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.8112 - val_loss: 0.5911 - val_accuracy: 0.8102\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.8435 - val_loss: 0.5660 - val_accuracy: 0.8427\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5490 - accuracy: 0.8657 - val_loss: 0.5415 - val_accuracy: 0.8642\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5236 - accuracy: 0.8831 - val_loss: 0.5170 - val_accuracy: 0.8777\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.8529 - accuracy: 0.3525 - val_loss: 0.8143 - val_accuracy: 0.4376\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7835 - accuracy: 0.4881 - val_loss: 0.7491 - val_accuracy: 0.5481\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.7252 - accuracy: 0.6023 - val_loss: 0.6927 - val_accuracy: 0.6362\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.6805 - val_loss: 0.6429 - val_accuracy: 0.6982\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6271 - accuracy: 0.7257 - val_loss: 0.5970 - val_accuracy: 0.7490\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.7636 - val_loss: 0.5530 - val_accuracy: 0.7895\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5396 - accuracy: 0.7977 - val_loss: 0.5108 - val_accuracy: 0.8237\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.8266 - val_loss: 0.4707 - val_accuracy: 0.8507\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.8512 - val_loss: 0.4338 - val_accuracy: 0.8761\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8756 - val_loss: 0.4000 - val_accuracy: 0.8967\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7903 - accuracy: 0.2781 - val_loss: 0.7536 - val_accuracy: 0.3521\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7339 - accuracy: 0.4058 - val_loss: 0.7037 - val_accuracy: 0.4865\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6866 - accuracy: 0.5495 - val_loss: 0.6607 - val_accuracy: 0.6184\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6455 - accuracy: 0.6764 - val_loss: 0.6232 - val_accuracy: 0.7242\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.7647 - val_loss: 0.5886 - val_accuracy: 0.7886\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5747 - accuracy: 0.8213 - val_loss: 0.5561 - val_accuracy: 0.8331\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.8569 - val_loss: 0.5254 - val_accuracy: 0.8593\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5122 - accuracy: 0.8782 - val_loss: 0.4962 - val_accuracy: 0.8831\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4834 - accuracy: 0.8927 - val_loss: 0.4687 - val_accuracy: 0.8975\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.9047 - val_loss: 0.4429 - val_accuracy: 0.9014\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7169 - accuracy: 0.4959 - val_loss: 0.6757 - val_accuracy: 0.5843\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.6312 - val_loss: 0.6177 - val_accuracy: 0.7154\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.7486 - val_loss: 0.5652 - val_accuracy: 0.7941\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7960 - val_loss: 0.5169 - val_accuracy: 0.8219\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8205 - val_loss: 0.4721 - val_accuracy: 0.8378\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8367 - val_loss: 0.4323 - val_accuracy: 0.8521\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8470 - val_loss: 0.3972 - val_accuracy: 0.8585\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8544 - val_loss: 0.3673 - val_accuracy: 0.8649\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8601 - val_loss: 0.3424 - val_accuracy: 0.8704\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8647 - val_loss: 0.3222 - val_accuracy: 0.8736\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "for kfold, (train, test) in enumerate(KFold(n_splits=10, \n",
    "                                shuffle=True).split(X_test_norm, y_test)):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # calling the model and compile it \n",
    "    seq_model = build_classifier('adam')\n",
    "\n",
    "    # run the model \n",
    "    seq_model.fit(X_test_norm.iloc[train], y_test.iloc[train],\n",
    "              batch_size=1000, epochs=10, validation_data=(X_test_norm.iloc[test], y_test.iloc[test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
